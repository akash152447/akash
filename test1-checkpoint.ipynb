{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df3d8f8f-aebc-4645-9f0a-32c6ba165017",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import warnings \n",
    "#warnings.filterwarning('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad4cb1d7-3c52-40bd-922c-366e9e460c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_price1=10\n",
    "product_price2=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1da333b-f1db-49d5-acd5-be4d004e5a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = dict({\"product_price1\" : 10, \"product_price2\":100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab8f632c-86b3-4b8d-ba3c-58610cd5508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Random: import random as rnd\n",
    "#2. Numpy: import numpy.random as npr\n",
    "#3. Tensorflow: import tensorflow.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba56b0d0-4d7f-4c37-ad35-56f5b34a1ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layers):\n",
    "        self.layers=layers\n",
    "        self.L = len(layers)\n",
    "        self.number_feature = layers[0]\n",
    "        self.number_class = layers[-1]\n",
    "     \n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "    \n",
    "        self.dw = {}\n",
    "        self.db = {} \n",
    "    \n",
    "        self.setup()\n",
    "def setup(self):\n",
    "    for i in range(1,self.L):\n",
    "        self.W[i] =tf.Variable(tf.random.normal(shape = (self.layers[i], self.layers[i-1])))\n",
    "        self.b[i] = tf.Variable(tf.random.normal(shape = (self.layers[i], 1))) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7de77a83-31d8-407f-b1d5-aec1f2ba4750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.dtype = \"float32\"\n",
    "# 2.dtype = np.float32\n",
    "# 3.dtype = tf.float32\n",
    "# 4.dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "324e6a11-62e0-4000-b63c-28e325ee355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def forwardpass(self, A):\n",
    "        A= tf.convert_to_tensor(A, dtype=float64)\n",
    "        for i in range(1, self.L):\n",
    "            z=tf.matmul(A, tf.transpose(self.W[i]))+tf.transpose(self.b[i])\n",
    "            if i!=self.L-1:\n",
    "                A=tf.nn.relu(Z)\n",
    "            else:\n",
    "                    A=Z\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1359ae5-cb3a-4bf9-9a2e-11bb7b290eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing loss  and updating the previous parameter\n",
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def compute_loss(self, A, Y):\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(Y,A))\n",
    "    \n",
    "    def upgrade_parameters(self,lr):\n",
    "        for j in range(1, self.L):\n",
    "            self.W[j].assing_sub(lr*self.dw[j])\n",
    "            self.b[j].assing_sub(lr*self.db[j])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "effb8007-7586-421e-ad67-6b2326c28be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def predict(self,x):\n",
    "        A= self.forwardpass(x)\n",
    "        return tf.argmax(tf.nn.softmax(A), axis=1)\n",
    "    def info(self):\n",
    "        num_params = 0\n",
    "        for i in range(1, self.L):\n",
    "            num_params += self.W[i].shape[0] * self.W[i].shape[1]\n",
    "            num_params += self.b[i].shape[0]\n",
    "        print(\"number of features: {}\" .format(self.number_feature))\n",
    "        print(\"total number of class: {}\".format(self.number_class))\n",
    "        \n",
    "        print(\"hidden layer info: {}\")\n",
    "        for j in range(1, self.L-1):\n",
    "            print(\"layer: {}, unit {}\" . format(i, self.layers[j]))\n",
    "        \n",
    "        print(\"total parameres:{}\" .format(num_params))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f22d2705-e4e3-4e2a-8310-a69d6ecbb9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d19a4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def tarinig_on_batch(x,y, lr):\n",
    "        x= tf.convert_to_tensor(x,dtype=tf.float64)\n",
    "        \n",
    "        y= tf.convert_to_tensor(y,dtype=tf.float64)\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            A=self.forwardpass(x)\n",
    "            loss= self.compute_loss(A,y)\n",
    "            \n",
    "        for key in self.W.keys():\n",
    "            self.dw[key] = tape.gardient(loss,self.W[key])\n",
    "            \n",
    "            self.db[key] = tape.gardient(loss,self.W[key])\n",
    "        del tape\n",
    "        \n",
    "        self.upgrade_parameters(lr)\n",
    "        \n",
    "        return loss.numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c4f543d-4c5d-4687-a866-b9ba0c499a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #training:\n",
    "\n",
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def train(self,x_train ,y_train, x_test, y_test, epochs, step_per_epochs, batch_size,lr):\n",
    "        history = {\"val_loss\":[],\n",
    "                  'train_loss':[],\n",
    "                   \"val_acc\":[]}\n",
    "        for e in range(e, epochs):\n",
    "            training_loss_epochs = 0.0\n",
    "            print('epoch {} '.format(e), end ='   ')\n",
    "            for i in range(step_per_epochs):\n",
    "                x_batch=x_train[i*batch_size:(i+1)*batch_size]\n",
    "                y_batch=y_train[i*batch_size:(i+1)*batch_size]\n",
    "                batch_loss=self.tarinig_on_batch(x_batch,y_batch, lr)\n",
    "                epochs_loss_train+=batch_loss\n",
    "                \n",
    "                if i% (step_per_epochs//10)==0:\n",
    "                    print(end=' . ')\n",
    "            history['train_loss'].append(epochs_loss_train/step_per_epochs)\n",
    "            \n",
    "            valA = self.forwardpass(x_test)\n",
    "            history['value_loss'].append(self.compute_loss(y_test,valA).numpy())\n",
    "        \n",
    "            valuePr = self.predict(x_test)\n",
    "            history['val_acc'].append(np.mean(np.argmax(y_test,axis=1)==valuePr.numpy()))\n",
    "        \n",
    "        \n",
    "            print('vale acc:' ,history['val_acc'][-1])\n",
    "        \n",
    "        return history\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630833c-664c-4ca6-bfc4-7019b3a43911",
   "metadata": {},
   "source": [
    "ASK FOR MEAN VARIANCE STANDARD DAVIATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339d3bf",
   "metadata": {},
   "source": [
    "# Data set loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "602e59a4-b91d-4baf-aae7-f9b0b8e7099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laodDatasets():\n",
    "    (x_train, y_train),(x_test,y_test)=tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    x_train= np.reshape(x_train, (x_train.shape[0],784))/255.0\n",
    "    y_train= tf.keras.utils.to_categorical(y_train)\n",
    "    x_test=np.reshape(x_test, (x_test.shape[0],784))/255.0\n",
    "    y_test=tf.keras.utils.to_categorical(y_test)\n",
    "    \n",
    "    print('x train datasets shape:{}'. format(x_train.shape))\n",
    "    print('y test datasets shape:{}'. format(x_test[0]))\n",
    "    \n",
    "    print('len of y test datasets: {}'.format(len(y_test[0])))\n",
    "    print('unique value:{}'.format(np.argmax(y_test[0])))\n",
    "     \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "def plot_random_sample(x,y,p=None):\n",
    "    indicies = np.random.choice(range(0, x.shape[0]),10)\n",
    "    y = np.argmax(y,axis=1)\n",
    "    \n",
    "    if p is None:\n",
    "        p=y\n",
    "        \n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i, index in enumerate(indicies):\n",
    "        \n",
    "        plt.subplot(2,5,i+1)\n",
    "        plt.imshow(x[index].reshape((28,28)),cmap='binary')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.gray()\n",
    "        \n",
    "        if y[index]==p[index]:\n",
    "            col='g'\n",
    "        else:\n",
    "            col='r'\n",
    "            \n",
    "        plt.xlabel(str(p[index]),color=col)\n",
    "        \n",
    "    return plt\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81925d45-f6bf-4336-96ed-ec1a70a4b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train datasets shape:(60000, 784)\n",
      "y test datasets shape:[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.32941176 0.7254902\n",
      " 0.62352941 0.59215686 0.23529412 0.14117647 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.87058824 0.99607843 0.99607843 0.99607843\n",
      " 0.99607843 0.94509804 0.77647059 0.77647059 0.77647059 0.77647059\n",
      " 0.77647059 0.77647059 0.77647059 0.77647059 0.66666667 0.20392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.2627451  0.44705882 0.28235294 0.44705882 0.63921569 0.89019608\n",
      " 0.99607843 0.88235294 0.99607843 0.99607843 0.99607843 0.98039216\n",
      " 0.89803922 0.99607843 0.99607843 0.54901961 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.06666667 0.25882353 0.05490196\n",
      " 0.2627451  0.2627451  0.2627451  0.23137255 0.08235294 0.9254902\n",
      " 0.99607843 0.41568627 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3254902  0.99215686 0.81960784 0.07058824\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.08627451\n",
      " 0.91372549 1.         0.3254902  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.50588235 0.99607843 0.93333333\n",
      " 0.17254902 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.23137255 0.97647059 0.99607843 0.24313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.52156863 0.99607843\n",
      " 0.73333333 0.01960784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.03529412 0.80392157 0.97254902 0.22745098 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.49411765\n",
      " 0.99607843 0.71372549 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.29411765 0.98431373 0.94117647 0.22352941\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.0745098\n",
      " 0.86666667 0.99607843 0.65098039 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.79607843 0.99607843 0.85882353\n",
      " 0.1372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.14901961 0.99607843 0.99607843 0.30196078 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.12156863 0.87843137 0.99607843\n",
      " 0.45098039 0.00392157 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.52156863 0.99607843 0.99607843 0.20392157 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.23921569 0.94901961\n",
      " 0.99607843 0.99607843 0.20392157 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.4745098  0.99607843 0.99607843 0.85882353\n",
      " 0.15686275 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4745098  0.99607843 0.81176471 0.07058824 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "len of y test datasets: 10\n",
      "unique value:7\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train),(x_test,y_test)=laodDatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89873db3-310d-41a6-b851-a565c4c2a086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
